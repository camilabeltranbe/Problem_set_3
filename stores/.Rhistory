#05_Pruebas_Julian
#Neural Networks
rm(list = ls())
library(pacman)  # Importemos Tydiverse que será necesario para lo que viene:
# Cargar las librerías listadas e instalarlas en caso de ser necesario
p_load( ISLR2,
rio, ## read datasets
sf, #datos espaciales
tidyverse, # manipular dataframes
tidymodels, #modelos de ML
nnet, # redes neuronales de una sola capa
spatialsample #validación cruzada espacial
)
wd <- ("C:/Users/Juan/Documents/Problem_set_33")
load("data_final.RData")
wd <- ("C:/Users/Juan/Documents/Problem_set_3")
load("data_final.RData")
wd <- ("C:/Users/Juan/Documents/Problem_set_3")
load("stores/data_final.RData")
setwd(paste0(wd,"/stores"))
load("data_final.RData")
## guardar las descripciones en un vector source
descriptions_train <- train$description
des_train_scource <- VectorSource(descriptions_train)
rm(list = ls())
library(pacman)  # Importemos Tydiverse que será necesario para lo que viene:
# Cargar las librerías listadas e instalarlas en caso de ser necesario
p_load( ISLR2,
rio, ## read datasets
sf, #datos espaciales
tidyverse, # manipular dataframes
tidymodels, #modelos de ML
nnet, # redes neuronales de una sola capa
spatialsample #validación cruzada espacial
)
wd <- ("C:/Users/Juan/Documents/Problem_set_3")
setwd(paste0(wd,"/stores"))
load("data_final.RData")
## guardar las descripciones en un vector source
descriptions_train <- train$description
des_train_scource <- VectorSource(descriptions_train)
rm(list = ls())
library(pacman)  # Importemos Tydiverse que será necesario para lo que viene:
# Cargar las librerías listadas e instalarlas en caso de ser necesario
p_load( ISLR2,
rio, ## read datasets
tidyverse, # Manipular dataframes
tm,   # para Text Mining
tidytext, #Para tokenización
stopwords,  # consultar stopwords
tidymodels,
sf,
spatialsample,
xgboost, #xgboosting
adabag, #adaboosting
dummy #para crear dummys
)
wd <- ("C:/Users/Juan/Documents/Problem_set_3")
setwd(paste0(wd,"/stores"))
load("data_final.RData")
descriptions_train <- train$description
des_train_scource <- VectorSource(descriptions_train)
descriptions_test <- test$description
des_test_scource <- VectorSource(descriptions_test)
descriptions_train <- train$description
des_train_scource <- VectorSource(descriptions_train)
descriptions_test <- test$description
des_test_scource <- VectorSource(descriptions_test)
# Make a volatile corpus: coffee_corpus
des_corpus_train <- VCorpus(des_train_scource, readerControl = list( language = "es"))
des_corpus_test <- VCorpus(des_test_scource, readerControl = list( language = "es"))
# Función para reemplazar números por palabras
reemplazar_numeros <- function(texto) {
palabras <- c("cero", "uno", "dos", "tres", "cuatro", "cinco", "seis", "siete", "ocho", "nueve", "diez")
# Reemplazar números del 0 al 10 por palabras
for (i in 0:10) {
texto <- gsub(sprintf("\\b%d\\b", i), palabras[i + 1], texto)}
return(texto)}
# Convertir texto a formato ASCII eliminando tildes y caracteres especiales
eliminar_tildes <- function(texto) {
texto_sin_tildes <- iconv(texto, "UTF-8", "ASCII", sub = "")
return(texto_sin_tildes)}
reemplazar_car_especiales <- function(texto) {
texto_sin_espe <-str_replace_all(texto, "[^[:alnum:]]", " ")
return(texto_sin_espe)}
## volver a las palabras a sus raíces
stem_espanol<-  function(texto) {
texto_stem <- stemDocument(texto, language="spanish")
return(texto_stem)}
# Descargamos la lista de las stopwords en español de dos fuentes diferentes y las combinamos
lista_palabras1 <- stopwords(language = "es", source = "snowball")
lista_palabras2 <- stopwords(language = "es", source = "nltk")
lista_palabras <- union(lista_palabras1, lista_palabras2)
lista_palabras<- union(lista_palabras,  c("vendo", "venta", "vende", "etc", "carrera", "calle", "casa", "apto", "apartamento",
"ubicado","ubicada") )
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, stripWhitespace) ## remover espacios en blanco
corpus <- tm_map(corpus, removePunctuation)  ## remover puntuacióm
corpus <- tm_map(corpus, content_transformer(tolower)) # todo minuscula
corpus <- tm_map(corpus, removeWords, c(lista_palabras)) # remover stopwords y otras que se quieran aádir
corpus<-  tm_map(corpus, content_transformer(reemplazar_numeros))  ## incluir funciones que nosotros creamos
corpus<-  tm_map(corpus, content_transformer(eliminar_tildes))  ## incluir funciones que nosotros creamos
corpus<-  tm_map(corpus, content_transformer(reemplazar_car_especiales))  ## incluir funciones que nosotros creamos
corpus<-  tm_map(corpus, content_transformer(stem_espanol))
corpus<-  tm_map(corpus, removeNumbers)  # remover numeros restantes
return(corpus)}
# apliquemos nuestra función de limpieza:
clean_des_train <- clean_corpus(des_corpus_train)
clean_des_test <- clean_corpus(des_corpus_test)
# crear la document - term matrix
description_dtm_train <- DocumentTermMatrix(clean_des_train)
description_dtm_test <- DocumentTermMatrix(clean_des_test)
#dejar en train solo variables que comparta con test
des_train <- as.data.frame(as.matrix(removeSparseTerms(description_dtm_train, 0.9), sparse=TRUE))
des_test <- as.data.frame(as.matrix(removeSparseTerms(description_dtm_test, 0.9), sparse=TRUE))
var_compartidas <- intersect(names(des_train), names(des_test))
des_train <- des_train[,var_compartidas]
des_test <- des_test[,var_compartidas]
# componentes principales eliminando las q tienen 90% de entradas nulas
pcdescriptions_train <- prcomp(as.matrix(des_train), scale=TRUE)
pcdescriptions_test <- prcomp(as.matrix(des_test), scale=TRUE)
zdes_train <- as.data.frame(predict(pcdescriptions_train)) %>%
mutate(property_id = train$property_id)
zdes_test <- as.data.frame(predict(pcdescriptions_test)) %>%
mutate(property_id = test$property_id)
des_train <- des_train %>%
mutate(property_id = train$property_id)
des_test <- des_test %>%
mutate(property_id = test$property_id)
# unir bases de datos de texto y de componentes principales
train_full<-  train %>%
full_join(des_train, join_by(property_id)) %>%
full_join(zdes_train, join_by(property_id))
test_full<-  test %>%
full_join(des_test, join_by(property_id)) %>%
full_join(zdes_test, join_by(property_id))
formula <- as.formula(
paste("Log_Precio ~ property_type + rooms3 + bathrooms3 +
surface_covered3 + n_pisos_numerico + localidad + Dist_pol + dist_parque + estrato + zona_t_g + abiert + ascensor + bbq + parqueader + gimnasi + deposit + chimene + comunal",
paste(pc_vars_train, collapse = "+"),  sep = "+"))
#full_formula_jp1<- as.formula(" Log_Precio ~ property_type + rooms3 + bathrooms3 +
#          surface_covered3 + n_pisos_numerico + localidad + Dist_pol + dist_parque + estrato + zona_t_g + abiert + ascensor + bbq + parqueader + gimnasi + deposit + chimene + comunal")
full_formula_jp1<- as.formula("Log_Precio ~ property_type + rooms3 + bathrooms3 +
surface_covered3 + n_pisos_numerico + localidad + Dist_pol + dist_parque + estrato + zona_t_g + abiert + ascensor + bbq + parqueader + gimnasi + deposit + chimene + comunal")
train_jp <- na.omit(train_full)
n <- nrow(train_jp)
lfit <-  lm(formula = formula, data = train_full)
#Formula para predicción
formula_n <- as.formula("Log_Precio ~ property_type + rooms3 + bathrooms3 +
surface_covered3 + n_pisos_numerico + localidad + Dist_pol + dist_parque + estrato + zona_t_g + abiert + ascensor + bbq + parqueader + gimnasi + deposit + chimene + comunal")
lfit <-  lm(formula = formula_n, data = train_full)
lpred <- predict(lfit, test_full)
with(train, mean(abs(lpred - Log_Precio)))
lpred
lpred <- predict(lfit, train_full)
with(train, mean(abs(lpred - Log_Precio)))
x <- scale(model.matrix(formula_n -1, data = train))
x <- scale(model.matrix(Log_Precio ~ property_type + rooms3 + bathrooms3 +
surface_covered3 + n_pisos_numerico + localidad + Dist_pol + dist_parque + estrato + zona_t_g + abiert + ascensor + bbq + parqueader + gimnasi + deposit + chimene + comunal - 1 , data = train))
x <- scale(model.matrix(Log_Precio ~ property_type + rooms3 + bathrooms3 +
surface_covered3 + n_pisos_numerico + localidad + Dist_pol + dist_parque + estrato + zona_t_g + abiert + ascensor + bbq + parqueader + gimnasi + deposit + chimene + comunal - 1 , data = train_full))
y <- train_full$Log_Precio
p_load( ISLR2,
glmnet,
rio, ## read datasets
tidyverse, # Manipular dataframes
tm,   # para Text Mining
tidytext, #Para tokenización
stopwords,  # consultar stopwords
tidymodels,
sf,
spatialsample,
xgboost, #xgboosting
adabag, #adaboosting
dummy #para crear dummys
)
cpred <- predict(cvfit, x, s = "lambda.min")
cvfit <- cv.glmnet(x, y, type.measure = "mae")
cpred <- predict(cvfit, x, s = "lambda.min")
mean(abs(y - cpred))
plot(cvfit)
install.packages("keras")
library(keras)
modnn <- keras_model_sequential() %>%
layer_dense(units = 50, activation = "relu",
input_shape = ncol(x)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 1)
x <- scale(model.matrix(Log_Precio ~ property_type + rooms3 + bathrooms3 +
surface_covered3 + n_pisos_numerico + localidad + Dist_pol + dist_parque + estrato + zona_t_g + abiert + ascensor + bbq + parqueader + gimnasi + deposit + chimene + comunal - 1 , data = train_full))
modnn
modnn <- keras_model_sequential() %>%
layer_dense(units = 50, activation = "relu",
input_shape = ncol(x)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 1)
reticulate::install_python(version = '<version>')
library(reticulate)
reticulate::install_python(version = '<version>')
use_python("C:/Users/Juan/anaconda3/pkgs/python-3.11.7-he1021f5_0/python.exe", required = TRUE)
use_python("C:/Users/Juan/anaconda3/pkgs/python-3.11.7-he1021f5_0", required = TRUE)
library(reticulate)
use_python("C:/Users/Juan/anaconda3/pkgs/python-3.11.7-he1021f5_0", required = TRUE)
library(reticulate)
use_python("C:\Users\Juan\anaconda3\pkgs\python-3.11.7-he1021f5_0", required = TRUE)
library(reticulate)
use_python("C:/Users/Juan/anaconda3/pkgs/python-3.11.7-he1021f5_0", required = TRUE)
library(reticulate)
use_python("C:/Users/Juan/anaconda3/pkgs/python-3.11.7-he1021f5_0/python.exe", required = TRUE)
library(reticulate)
use_condaenv("r-keras", required = TRUE)
library(reticulate)
py_config()
library(keras)
modnn <- keras_model_sequential() %>%
layer_dense(units = 50, activation = "relu",
input_shape = ncol(x)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 1)
library(pacman)  # Importemos Tydiverse que será necesario para lo que viene:
# Cargar las librerías listadas e instalarlas en caso de ser necesario
p_load( ISLR2,
glmnet,
rio, ## read datasets
tidyverse, # Manipular dataframes
tm,   # para Text Mining
tidytext, #Para tokenización
stopwords,  # consultar stopwords
tidymodels,
sf,
spatialsample,
xgboost, #xgboosting
adabag, #adaboosting
dummy #para crear dummys
)
library(keras)
library(reticulate)
use_python("C:/ruta/a/python.exe", required = TRUE)
library(reticulate)
use_python("C:/Users/Juan/AppData/Roaming/Microsoft/Windows/Start Menu/Programs/Python 3.12", required = TRUE)
library(reticulate)
py_config()
library(keras)
use_python("C:/Users/Juan/AppData/Roaming/Microsoft/Windows/Start Menu/Programs/Python 3.12", required = TRUE)
